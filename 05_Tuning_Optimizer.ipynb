{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning SGD Optimizer for Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "%matplotlib inline\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model\n",
    "#https://github.com/jrosebr1/imutils\n",
    "from imutils import paths\n",
    "import imutils\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "import os\n",
    "import pickle\n",
    "from tensorflow import reset_default_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet:\n",
    "    #optimizer to use should be 'sgd'\n",
    "    @staticmethod\n",
    "    def build(width, height, depth, classes):\n",
    "        model = Sequential()\n",
    "        #input shape is 227x227x3\n",
    "        inputShape = (height, width, depth)\n",
    "        model.add(Conv2D(filters=96, kernel_size=11, strides=4,\n",
    "                         input_shape=inputShape, activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(3,3), strides=(2,2)))\n",
    "        model.add(Conv2D(filters=256, kernel_size=5, strides=1,\n",
    "                         padding='same', activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(3,3), strides=(2,2)))\n",
    "        model.add(Conv2D(filters=384, kernel_size=3, strides=1,\n",
    "                         padding='same', activation='relu'))\n",
    "        model.add(Conv2D(filters=256, kernel_size=3, strides=1,\n",
    "                         padding='same', activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(3,3), strides=(2,2)))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(4096, activation='relu'))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(4096, activation='relu'))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(classes, activation='softmax'))\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(width, height, path):\n",
    "    \"\"\"\n",
    "    Resize and rescale images stored in image folder.\n",
    "    \n",
    "    Return pre-processed data and labels for the classes based\n",
    "    on sub-directories in the image folder\n",
    "    \"\"\"\n",
    "    #containers for pre-processed image data and class labels\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    #images directory contains 3 sub-directories: 'poison_ivy', 'poison_oak', 'poison_sumac'\n",
    "    #randomly get image paths and shuffle them\n",
    "    # current path 'C:\\\\Users\\\\jltsa\\\\Desktop\\\\Project_2\\\\images'\n",
    "    image_paths = sorted(list(paths.list_images(path)))\n",
    "    random.seed(42)\n",
    "    random.shuffle(image_paths)\n",
    "\n",
    "    #preprocess images to width x height pixels as required for LeNet\n",
    "    for image_path in image_paths:\n",
    "        image = cv2.imread(image_path)\n",
    "        if image is not None:\n",
    "            image = cv2.resize(image, (width, height))\n",
    "            image = img_to_array(image)\n",
    "            data.append(image)\n",
    "    \n",
    "            #Extract class labels\n",
    "            label = image_path.split(os.path.sep)[-2]\n",
    "            if label == 'poison_ivy':\n",
    "                label = 0\n",
    "            elif label == 'poison_oak':\n",
    "                label = 1\n",
    "            else:\n",
    "                label = 2\n",
    "            labels.append(label)\n",
    "       \n",
    "    #Scal pixel intensities from 0 to 1\n",
    "    data = np.array(data, dtype='float') / 255.0\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_acc_loss(epochs, model, title=None, save=False, path=None, save_as=None):\n",
    "    \"\"\"\n",
    "    plot the accuracy and loss of the train and test data\n",
    "    save plot optional\n",
    "    \"\"\"\n",
    "    plt.style.use('ggplot')\n",
    "    plt.figure()\n",
    "    plt.plot(np.arange(0,epochs), model.history.history['loss'], label='Train_loss')\n",
    "    plt.plot(np.arange(0,epochs), model.history.history['val_loss'], label='Val_loss')\n",
    "    plt.plot(np.arange(0,epochs), model.history.history['acc'], label='Train_acc')\n",
    "    plt.plot(np.arange(0,epochs), model.history.history['val_acc'], label='Val_acc')\n",
    "    plt.title('Training Loss and Accuracy' + ' ' + str(title))\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss/Acc')\n",
    "    plt.legend(loc='upper left')\n",
    "    \n",
    "    if save == True:\n",
    "        plt.savefig(path+'\\\\' + str(save_as) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metric(epochs, metric, y_label, title=None, save=False, path=None, save_as=None):\n",
    "    \"\"\"\n",
    "    plot single metric\n",
    "    save plot optional\n",
    "    \"\"\"\n",
    "    plt.style.use('ggplot')\n",
    "    plt.figure()\n",
    "    for met in range(len(metric)):\n",
    "        plt.plot(np.arange(0,epochs), metric[met], label=y_label+str(met))\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(y_label)\n",
    "    plt.legend(loc='upper left')\n",
    "    \n",
    "    if save == True:\n",
    "        plt.savefig(str(path)+'\\\\' + str(save_as) + '.png')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To increase the amount of training data, build an image generator using data augmentation\n",
    "aug_gen = ImageDataGenerator(rotation_range=30, width_shift_range=0.1,\n",
    "                            height_shift_range=0.1, shear_range=0.2,\n",
    "                            zoom_range=0.2, horizontal_flip=True, fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize global training variables\n",
    "\n",
    "EPOCHS = 30\n",
    "#Learning rate\n",
    "#LR = 1e-3\n",
    "#Batch Size\n",
    "BS = 15\n",
    "path_to_img = 'C:\\\\Users\\\\jltsa\\\\Desktop\\\\Project_2\\\\images'\n",
    "path_to_models = 'models\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\jltsa\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "sgd1 = SGD(lr=0.01, momentum=0.5, nesterov=True)\n",
    "sgd2 = SGD(lr=0.01, momentum=0.8, nesterov=True)\n",
    "sgd3 = SGD(lr=0.001, momentum=0.5, nesterov=True)\n",
    "sgd4 = SGD(lr=0.001, momentum=0.8, nesterov=True)\n",
    "sgd5 = SGD(lr=0.0001, momentum=0.5, nesterov=True)\n",
    "sgd6 = SGD(lr=0.0001, momentum=0.8, nesterov=True)\n",
    "\n",
    "opts = [sgd1, sgd2, sgd3, sgd4, sgd5, sgd6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = [[],[],[],[],[],[]]\n",
    "val_loss = [[],[],[],[],[],[]]\n",
    "acc = [[],[],[],[],[],[]]\n",
    "val_acc = [[],[],[],[],[],[]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(im_path, model_path, optimizer, weights):\n",
    "\n",
    "    data, labels = pre_process(227, 227, im_path)\n",
    "    #Split data into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.25, random_state=42)\n",
    "    #One hot encoding\n",
    "    y_train = to_categorical(y_train)\n",
    "    y_test = to_categorical(y_test)\n",
    "    \n",
    "\n",
    "    #create a checkpoint to store the best weights of the model\n",
    "    #to use these weights later, initialize the same model architecture that the weights were trained from\n",
    "    #then call model.load_weights('best_weights_alex.h5')\n",
    "    #can make predictions model.predict_classes(data)\n",
    "    model_path = model_path+weights+\".h5\"\n",
    "    checkpoint = ModelCheckpoint(model_path, monitor='val_loss', save_best_only=True)\n",
    "    #add early stopping if \n",
    "    earlystop = EarlyStopping(monitor='val_loss', patience=3)\n",
    "        \n",
    "    callbacks_list=[checkpoint, earlystop]\n",
    "        \n",
    "    #Initialize model\n",
    "    model = AlexNet.build(width=227, height=227, depth=3, classes=3)\n",
    "    #opt_alex = Adam(lr=LR, decay=LR / EPOCHS)\n",
    "\n",
    "    #if model has 2 classes use loss='binary_crossentropy'\n",
    "    #AlexNet performs better with 'sgd' optimizer\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    #fit model using a generator to increase variation and amount of data\n",
    "    model.fit_generator(aug_gen.flow(X_train, y_train, batch_size=BS),\n",
    "                    validation_data=(X_test, y_test), steps_per_epoch=len(X_train // BS),\n",
    "                    epochs=EPOCHS, callbacks=callbacks_list, verbose=1)\n",
    "    #save history for loss and val scores\n",
    "    loss.append(model.history.history['loss'])\n",
    "    val_loss.append(model.history.history['val_loss'])\n",
    "    acc.append(model.history.history['acc'])\n",
    "    val_acc.append(model.history.history['val_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "543/543 [==============================] - 93s 171ms/step - loss: 0.9954 - acc: 0.4912 - val_loss: 0.9663 - val_acc: 0.5138\n",
      "Epoch 2/30\n",
      "543/543 [==============================] - 87s 159ms/step - loss: 0.9601 - acc: 0.5359 - val_loss: 0.9958 - val_acc: 0.5083\n",
      "Epoch 3/30\n",
      "543/543 [==============================] - 87s 161ms/step - loss: 0.9431 - acc: 0.5422 - val_loss: 0.9175 - val_acc: 0.5580\n",
      "Epoch 4/30\n",
      "543/543 [==============================] - 88s 163ms/step - loss: 0.9241 - acc: 0.5522 - val_loss: 0.9010 - val_acc: 0.6022\n",
      "Epoch 5/30\n",
      "543/543 [==============================] - 87s 160ms/step - loss: 0.8952 - acc: 0.5651 - val_loss: 0.9266 - val_acc: 0.5746\n",
      "Epoch 6/30\n",
      "543/543 [==============================] - 87s 159ms/step - loss: 0.8496 - acc: 0.5860 - val_loss: 0.8217 - val_acc: 0.6298\n",
      "Epoch 7/30\n",
      "543/543 [==============================] - 88s 162ms/step - loss: 0.7705 - acc: 0.6361 - val_loss: 0.9332 - val_acc: 0.5801\n",
      "Epoch 8/30\n",
      "543/543 [==============================] - 87s 160ms/step - loss: 0.7067 - acc: 0.6744 - val_loss: 0.9599 - val_acc: 0.6133\n",
      "Epoch 9/30\n",
      "543/543 [==============================] - 87s 160ms/step - loss: 0.6209 - acc: 0.7234 - val_loss: 1.0478 - val_acc: 0.4696\n"
     ]
    }
   ],
   "source": [
    "train_model(path_to_img, path_to_models, sgd1, \"SGD1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "i =[]\n",
    "i.append(acc[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.4890309785313739,\n",
       "  0.5339610954630847,\n",
       "  0.5408663058853419,\n",
       "  0.551836546161606,\n",
       "  0.5658506132227328,\n",
       "  0.5897049762491003,\n",
       "  0.6364548250489237,\n",
       "  0.6760828792936608,\n",
       "  0.7261770395572577]]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('metrics\\\\'+'acc.pickle', 'wb') as file:\n",
    "            pickle.dump(i, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('metrics\\\\'+'val_loss.pickle', 'rb') as file:\n",
    "    e = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.9663126679415203,\n",
       "  0.9957705831659432,\n",
       "  0.9174511067116458,\n",
       "  0.901037379852316,\n",
       "  0.9265976604177149,\n",
       "  0.8217157069490759,\n",
       "  0.9331987885480427,\n",
       "  0.9599107991265987,\n",
       "  1.0477588515913947]]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "e.append(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('metrics\\\\'+'loss.pickle', 'wb') as file:\n",
    "            pickle.dump(e, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.993493871632354], [0.9922933940288491], 3]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('metrics\\\\'+'loss.pickle', 'rb') as file:\n",
    "    e = pickle.load(file)\n",
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
