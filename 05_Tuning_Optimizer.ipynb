{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning SGD Optimizer for Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "%matplotlib inline\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model\n",
    "#https://github.com/jrosebr1/imutils\n",
    "from imutils import paths\n",
    "import imutils\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet:\n",
    "    #optimizer to use should be 'sgd'\n",
    "    @staticmethod\n",
    "    def build(width, height, depth, classes):\n",
    "        model = Sequential()\n",
    "        #input shape is 227x227x3\n",
    "        inputShape = (height, width, depth)\n",
    "        model.add(Conv2D(filters=96, kernel_size=11, strides=4,\n",
    "                         input_shape=inputShape, activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(3,3), strides=(2,2)))\n",
    "        model.add(Conv2D(filters=256, kernel_size=5, strides=1,\n",
    "                         padding='same', activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(3,3), strides=(2,2)))\n",
    "        model.add(Conv2D(filters=384, kernel_size=3, strides=1,\n",
    "                         padding='same', activation='relu'))\n",
    "        model.add(Conv2D(filters=256, kernel_size=3, strides=1,\n",
    "                         padding='same', activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(3,3), strides=(2,2)))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(4096, activation='relu'))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(4096, activation='relu'))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(classes, activation='softmax'))\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(width, height, path):\n",
    "    \"\"\"\n",
    "    Resize and rescale images stored in image folder.\n",
    "    \n",
    "    Return pre-processed data and labels for the classes based\n",
    "    on sub-directories in the image folder\n",
    "    \"\"\"\n",
    "    #containers for pre-processed image data and class labels\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    #images directory contains 3 sub-directories: 'poison_ivy', 'poison_oak', 'poison_sumac'\n",
    "    #randomly get image paths and shuffle them\n",
    "    # current path 'C:\\\\Users\\\\jltsa\\\\Desktop\\\\Project_2\\\\images'\n",
    "    image_paths = sorted(list(paths.list_images(path)))\n",
    "    random.seed(42)\n",
    "    random.shuffle(image_paths)\n",
    "\n",
    "    #preprocess images to width x height pixels as required for LeNet\n",
    "    for image_path in image_paths:\n",
    "        image = cv2.imread(image_path)\n",
    "        if image is not None:\n",
    "            image = cv2.resize(image, (width, height))\n",
    "            image = img_to_array(image)\n",
    "            data.append(image)\n",
    "    \n",
    "            #Extract class labels\n",
    "            label = image_path.split(os.path.sep)[-2]\n",
    "            if label == 'poison_ivy':\n",
    "                label = 0\n",
    "            elif label == 'poison_oak':\n",
    "                label = 1\n",
    "            else:\n",
    "                label = 2\n",
    "            labels.append(label)\n",
    "       \n",
    "    #Scal pixel intensities from 0 to 1\n",
    "    data = np.array(data, dtype='float') / 255.0\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_acc_loss(epochs, model, title=None, save=False, path=None, save_as=None):\n",
    "    \"\"\"\n",
    "    plot the accuracy and loss of the train and test data\n",
    "    save plot optional\n",
    "    \"\"\"\n",
    "    plt.style.use('ggplot')\n",
    "    plt.figure()\n",
    "    plt.plot(np.arange(0,epochs), model.history['loss'], label='Train_loss')\n",
    "    plt.plot(np.arange(0,epochs), model.history['val_loss'], label='Val_loss')\n",
    "    plt.plot(np.arange(0,epochs), model.history['acc'], label='Train_acc')\n",
    "    plt.plot(np.arange(0,epochs), model.history['val_acc'], label='Val_acc')\n",
    "    plt.title('Training Loss and Accuracy' + ' ' + str(title))\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss/Acc')\n",
    "    plt.legend(loc='upper left')\n",
    "    \n",
    "    if save == True:\n",
    "        plt.savefig(str(path)+'\\\\' + str(save_as) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metric(epochs, metric, y_label, title=None, save=False, path=None, save_as=None):\n",
    "    \"\"\"\n",
    "    plot single metric\n",
    "    save plot optional\n",
    "    \"\"\"\n",
    "    plt.style.use('ggplot')\n",
    "    plt.figure()\n",
    "    for met in range(len(metric)):\n",
    "        plt.plot(np.arange(0,epochs), metric[met], label=y_label+str(met))\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(y_label)\n",
    "    plt.legend(loc='upper left')\n",
    "    \n",
    "    if save == True:\n",
    "        plt.savefig(str(path)+'\\\\' + str(save_as) + '.png')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To increase the amount of training data, build an image generator using data augmentation\n",
    "aug_gen = ImageDataGenerator(rotation_range=30, width_shift_range=0.1,\n",
    "                            height_shift_range=0.1, shear_range=0.2,\n",
    "                            zoom_range=0.2, horizontal_flip=True, fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize global training variables\n",
    "\n",
    "EPOCHS = 2\n",
    "#Learning rate\n",
    "#LR = 1e-3\n",
    "#Batch Size\n",
    "BS = 15\n",
    "path_to_img = 'C:\\\\Users\\\\jltsa\\\\Desktop\\\\Project_2\\\\images'\n",
    "path_to_models = 'models\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd1 = SGD(lr=0.01, momentum=0.5, nesterov=True)\n",
    "sgd2 = SGD(lr=0.01, momentum=0.8, nesterov=True)\n",
    "sgd3 = SGD(lr=0.001, momentum=0.5, nesterov=True)\n",
    "sgd4 = SGD(lr=0.001, momentum=0.8, nesterov=True)\n",
    "sgd5 = SGD(lr=0.0001, momentum=0.5, nesterov=True)\n",
    "sgd6 = SGD(lr=0.0001, momentum=0.8, nesterov=True)\n",
    "\n",
    "opts = [sgd1, sgd2, sgd3, sgd4, sgd5, sgd6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(im_path, model_path, optimizers):\n",
    "    loss = []\n",
    "    acc = []\n",
    "    val_loss = []\n",
    "    val_acc = []\n",
    "    \n",
    "    data, labels = pre_process(227, 227, im_path)\n",
    "    #Split data into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.25, random_state=42)\n",
    "    #One hot encoding\n",
    "    y_train = to_categorical(y_train)\n",
    "    y_test = to_categorical(y_test)\n",
    "    \n",
    "    callbacks_list=[checkpoint, earlystop]\n",
    "    \n",
    "    for opt in range(len(optimizers)):\n",
    "        #create a checkpoint to store the best weights of the model\n",
    "        #to use these weights later, initialize the same model architecture that the weights were trained from\n",
    "        #then call model.load_weights('best_weights_alex.h5')\n",
    "        #can make predictions model.predict_classes(data)\n",
    "        model_path += model_path+\"_test\"+str(opt)\n",
    "        checkpoint = ModelCheckpoint(model_path, monitor='val_loss', save_best_only=True)\n",
    "        #add early stopping if \n",
    "        earlystop = EarlyStopping(monitor='val_loss', patience=3)\n",
    "        \n",
    "        \n",
    "        #Initialize model\n",
    "        model = AlexNet.build(width=227, height=227, depth=3, classes=3)\n",
    "        #opt_alex = Adam(lr=LR, decay=LR / EPOCHS)\n",
    "\n",
    "        #if model has 2 classes use loss='binary_crossentropy'\n",
    "        #AlexNet performs better with 'sgd' optimizer\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=optimizers[opt], metrics=['accuracy'])\n",
    "\n",
    "        #fit model using a generator to increase variation and amount of data\n",
    "        model.fit_generator(aug_gen.flow(X_train, y_train, batch_size=BS),\n",
    "                       validation_data=(X_test, y_test), steps_per_epoch=len(X_train // BS),\n",
    "                       epochs=EPOCHS, callbacks=callbacks_list, verbose=1)\n",
    "        #save history for loss and val scores\n",
    "        loss.append(model.history['loss'])\n",
    "        val_loss.append(model.history['val_loss'])\n",
    "        acc.append(model.history['acc'])\n",
    "        val_acc.append(model.history['val_acc'])\n",
    "\n",
    "        #title = \"AlexNet Using SGD\"+str(opt)\n",
    "        title = \"test\"+str(opt)\n",
    "        plot_acc_loss(EPOCHS, model, title=title, save=True, path='plots', save_as=\"test\"+str(opt))\n",
    " \n",
    "    return loss, acc, val_loss, val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexNet Using SGD0\n",
      "AlexNet Using SGD1\n",
      "AlexNet Using SGD2\n",
      "AlexNet Using SGD3\n",
      "AlexNet Using SGD4\n",
      "AlexNet Using SGD5\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metric(epochs, metric, 'Accuracy', title=\"Accuracies\", save=False, path='plots', save_as=\"SGD_accs\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
