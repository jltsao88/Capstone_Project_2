{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning SGD Optimizer for Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "%matplotlib inline\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model\n",
    "#https://github.com/jrosebr1/imutils\n",
    "from imutils import paths\n",
    "import imutils\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "import os\n",
    "import pickle\n",
    "from numba import cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet:\n",
    "    #optimizer to use should be 'sgd'\n",
    "    @staticmethod\n",
    "    def build(width, height, depth, classes):\n",
    "        model = Sequential()\n",
    "        #input shape is 227x227x3\n",
    "        inputShape = (height, width, depth)\n",
    "        model.add(Conv2D(filters=96, kernel_size=11, strides=4,\n",
    "                         input_shape=inputShape, activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(3,3), strides=(2,2)))\n",
    "        model.add(Conv2D(filters=256, kernel_size=5, strides=1,\n",
    "                         padding='same', activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(3,3), strides=(2,2)))\n",
    "        model.add(Conv2D(filters=384, kernel_size=3, strides=1,\n",
    "                         padding='same', activation='relu'))\n",
    "        model.add(Conv2D(filters=256, kernel_size=3, strides=1,\n",
    "                         padding='same', activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(3,3), strides=(2,2)))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(4096, activation='relu'))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(4096, activation='relu'))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(classes, activation='softmax'))\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(width, height, path):\n",
    "    \"\"\"\n",
    "    Resize and rescale images stored in image folder.\n",
    "    \n",
    "    Return pre-processed data and labels for the classes based\n",
    "    on sub-directories in the image folder\n",
    "    \"\"\"\n",
    "    #containers for pre-processed image data and class labels\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    #images directory contains 3 sub-directories: 'poison_ivy', 'poison_oak', 'poison_sumac'\n",
    "    #randomly get image paths and shuffle them\n",
    "    # current path 'C:\\\\Users\\\\jltsa\\\\Desktop\\\\Project_2\\\\images'\n",
    "    image_paths = sorted(list(paths.list_images(path)))\n",
    "    random.seed(42)\n",
    "    random.shuffle(image_paths)\n",
    "\n",
    "    #preprocess images to width x height pixels as required for LeNet\n",
    "    for image_path in image_paths:\n",
    "        image = cv2.imread(image_path)\n",
    "        if image is not None:\n",
    "            image = cv2.resize(image, (width, height))\n",
    "            image = img_to_array(image)\n",
    "            data.append(image)\n",
    "    \n",
    "            #Extract class labels\n",
    "            label = image_path.split(os.path.sep)[-2]\n",
    "            if label == 'poison_ivy':\n",
    "                label = 0\n",
    "            elif label == 'poison_oak':\n",
    "                label = 1\n",
    "            else:\n",
    "                label = 2\n",
    "            labels.append(label)\n",
    "       \n",
    "    #Scal pixel intensities from 0 to 1\n",
    "    data = np.array(data, dtype='float') / 255.0\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_acc_loss(epochs, model, title=None, save=False, path=None, save_as=None):\n",
    "    \"\"\"\n",
    "    plot the accuracy and loss of the train and test data\n",
    "    save plot optional\n",
    "    \"\"\"\n",
    "    plt.style.use('ggplot')\n",
    "    plt.figure()\n",
    "    plt.plot(np.arange(0,epochs), model.history.history['loss'], label='Train_loss')\n",
    "    plt.plot(np.arange(0,epochs), model.history.history['val_loss'], label='Val_loss')\n",
    "    plt.plot(np.arange(0,epochs), model.history.history['acc'], label='Train_acc')\n",
    "    plt.plot(np.arange(0,epochs), model.history.history['val_acc'], label='Val_acc')\n",
    "    plt.title('Training Loss and Accuracy' + ' ' + str(title))\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss/Acc')\n",
    "    plt.legend(loc='upper left')\n",
    "    \n",
    "    if save == True:\n",
    "        plt.savefig(path+'\\\\' + str(save_as) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metric(epochs, metric, y_label, title=None, save=False, path=None, save_as=None):\n",
    "    \"\"\"\n",
    "    plot single metric\n",
    "    save plot optional\n",
    "    \"\"\"\n",
    "    plt.style.use('ggplot')\n",
    "    plt.figure()\n",
    "    for met in range(len(metric)):\n",
    "        plt.plot(np.arange(0,epochs), metric[met], label=y_label+str(met))\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(y_label)\n",
    "    plt.legend(loc='upper left')\n",
    "    \n",
    "    if save == True:\n",
    "        plt.savefig(str(path)+'\\\\' + str(save_as) + '.png')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To increase the amount of training data, build an image generator using data augmentation\n",
    "aug_gen = ImageDataGenerator(rotation_range=30, width_shift_range=0.1,\n",
    "                            height_shift_range=0.1, shear_range=0.2,\n",
    "                            zoom_range=0.2, horizontal_flip=True, fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize global training variables\n",
    "\n",
    "EPOCHS = 2\n",
    "#Learning rate\n",
    "#LR = 1e-3\n",
    "#Batch Size\n",
    "BS = 15\n",
    "path_to_img = 'C:\\\\Users\\\\jltsa\\\\Desktop\\\\Project_2\\\\images'\n",
    "path_to_models = 'models\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\jltsa\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "sgd1 = SGD(lr=0.01, momentum=0.5, nesterov=True)\n",
    "sgd2 = SGD(lr=0.01, momentum=0.8, nesterov=True)\n",
    "sgd3 = SGD(lr=0.001, momentum=0.5, nesterov=True)\n",
    "sgd4 = SGD(lr=0.001, momentum=0.8, nesterov=True)\n",
    "sgd5 = SGD(lr=0.0001, momentum=0.5, nesterov=True)\n",
    "sgd6 = SGD(lr=0.0001, momentum=0.8, nesterov=True)\n",
    "\n",
    "opts = [sgd1, sgd2, sgd3, sgd4, sgd5, sgd6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(im_path, model_path, optimizers):\n",
    "    loss = []\n",
    "    val_loss =[]\n",
    "    acc = []\n",
    "    val_acc =[]\n",
    "    \n",
    "    data, labels = pre_process(227, 227, im_path)\n",
    "    #Split data into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.25, random_state=42)\n",
    "    #One hot encoding\n",
    "    y_train = to_categorical(y_train)\n",
    "    y_test = to_categorical(y_test)\n",
    "    \n",
    "    for opt in range(len(optimizers)):\n",
    "        #create a checkpoint to store the best weights of the model\n",
    "        #to use these weights later, initialize the same model architecture that the weights were trained from\n",
    "        #then call model.load_weights('best_weights_alex.h5')\n",
    "        #can make predictions model.predict_classes(data)\n",
    "        model_path = model_path+\"test\"+str(opt)+\".h5\"\n",
    "        checkpoint = ModelCheckpoint(model_path, monitor='val_loss', save_best_only=True)\n",
    "        #add early stopping if \n",
    "        earlystop = EarlyStopping(monitor='val_loss', patience=3)\n",
    "        \n",
    "        callbacks_list=[checkpoint, earlystop]\n",
    "        \n",
    "        #Initialize model\n",
    "        model = AlexNet.build(width=227, height=227, depth=3, classes=3)\n",
    "        #opt_alex = Adam(lr=LR, decay=LR / EPOCHS)\n",
    "\n",
    "        #if model has 2 classes use loss='binary_crossentropy'\n",
    "        #AlexNet performs better with 'sgd' optimizer\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=optimizers[opt], metrics=['accuracy'])\n",
    "\n",
    "        #fit model using a generator to increase variation and amount of data\n",
    "        model.fit_generator(aug_gen.flow(X_train, y_train, batch_size=BS),\n",
    "                    validation_data=(X_test, y_test), steps_per_epoch=len(X_train // BS),\n",
    "                    epochs=EPOCHS, callbacks=callbacks_list, verbose=1)\n",
    "        #save history for loss and val scores\n",
    "        loss.append(model.history.history['loss'])\n",
    "        val_loss.append(model.history.history['val_loss'])\n",
    "        acc.append(model.history.history['acc'])\n",
    "        val_acc.append(model.history.history['val_acc'])\n",
    "         \n",
    "        title='SGD'+str(opt)\n",
    "        plot_acc_loss(EPOCHS, model, title=title, save=True, path='plots', save_as='SGD'+str(opt))\n",
    "        \n",
    "        #free up GPU memory\n",
    "        cuda.select_device(0)\n",
    "        cuda.close()\n",
    "    return loss, val_loss, acc, val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\jltsa\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\Users\\jltsa\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/2\n",
      "447/543 [=======================>......] - ETA: 15s - loss: 0.9994 - acc: 0.4914"
     ]
    }
   ],
   "source": [
    "loss, val_loss, acc, val_acc = train_model(path_to_img, path_to_models, opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metric(epochs, metric, 'Accuracy', title=\"Accuracies\", save=False, path='plots', save_as=\"SGD_accs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('metrics\\\\'+'test0.pickle', 'rb') as file:\n",
    "            e = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
