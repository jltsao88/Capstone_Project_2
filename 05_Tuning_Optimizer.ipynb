{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning SGD Optimizer for Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "%matplotlib inline\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model\n",
    "#https://github.com/jrosebr1/imutils\n",
    "from imutils import paths\n",
    "import imutils\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "import os\n",
    "import pickle\n",
    "from tensorflow import reset_default_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet:\n",
    "    #optimizer to use should be 'sgd'\n",
    "    @staticmethod\n",
    "    def build(width, height, depth, classes):\n",
    "        model = Sequential()\n",
    "        #input shape is 227x227x3\n",
    "        inputShape = (height, width, depth)\n",
    "        model.add(Conv2D(filters=96, kernel_size=11, strides=4,\n",
    "                         input_shape=inputShape, activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(3,3), strides=(2,2)))\n",
    "        model.add(Conv2D(filters=256, kernel_size=5, strides=1,\n",
    "                         padding='same', activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(3,3), strides=(2,2)))\n",
    "        model.add(Conv2D(filters=384, kernel_size=3, strides=1,\n",
    "                         padding='same', activation='relu'))\n",
    "        model.add(Conv2D(filters=256, kernel_size=3, strides=1,\n",
    "                         padding='same', activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(3,3), strides=(2,2)))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(4096, activation='relu'))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(4096, activation='relu'))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(classes, activation='softmax'))\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(width, height, path):\n",
    "    \"\"\"\n",
    "    Resize and rescale images stored in image folder.\n",
    "    \n",
    "    Return pre-processed data and labels for the classes based\n",
    "    on sub-directories in the image folder\n",
    "    \"\"\"\n",
    "    #containers for pre-processed image data and class labels\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    #images directory contains 3 sub-directories: 'poison_ivy', 'poison_oak', 'poison_sumac'\n",
    "    #randomly get image paths and shuffle them\n",
    "    # current path 'C:\\\\Users\\\\jltsa\\\\Desktop\\\\Project_2\\\\images'\n",
    "    image_paths = sorted(list(paths.list_images(path)))\n",
    "    random.seed(42)\n",
    "    random.shuffle(image_paths)\n",
    "\n",
    "    #preprocess images to width x height pixels as required for LeNet\n",
    "    for image_path in image_paths:\n",
    "        image = cv2.imread(image_path)\n",
    "        if image is not None:\n",
    "            image = cv2.resize(image, (width, height))\n",
    "            image = img_to_array(image)\n",
    "            data.append(image)\n",
    "    \n",
    "            #Extract class labels\n",
    "            label = image_path.split(os.path.sep)[-2]\n",
    "            if label == 'poison_ivy':\n",
    "                label = 0\n",
    "            elif label == 'poison_oak':\n",
    "                label = 1\n",
    "            else:\n",
    "                label = 2\n",
    "            labels.append(label)\n",
    "       \n",
    "    #Scal pixel intensities from 0 to 1\n",
    "    data = np.array(data, dtype='float') / 255.0\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_acc_loss(epochs, model, title=None, save=False, path=None, save_as=None):\n",
    "    \"\"\"\n",
    "    plot the accuracy and loss of the train and test data\n",
    "    save plot optional\n",
    "    \"\"\"\n",
    "    plt.style.use('ggplot')\n",
    "    plt.figure()\n",
    "    plt.plot(np.arange(0,epochs), model.history.history['loss'], label='Train_loss')\n",
    "    plt.plot(np.arange(0,epochs), model.history.history['val_loss'], label='Val_loss')\n",
    "    plt.plot(np.arange(0,epochs), model.history.history['acc'], label='Train_acc')\n",
    "    plt.plot(np.arange(0,epochs), model.history.history['val_acc'], label='Val_acc')\n",
    "    plt.title('Training Loss and Accuracy' + ' ' + str(title))\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss/Acc')\n",
    "    plt.legend(loc='upper left')\n",
    "    \n",
    "    if save == True:\n",
    "        plt.savefig(path+'\\\\' + str(save_as) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metric(epochs, metric, y_label, title=None, save=False, path=None, save_as=None):\n",
    "    \"\"\"\n",
    "    plot single metric\n",
    "    save plot optional\n",
    "    \"\"\"\n",
    "    plt.style.use('ggplot')\n",
    "    plt.figure()\n",
    "    for met in range(len(metric)):\n",
    "        plt.plot(np.arange(0,epochs), metric[met], label=y_label+str(met))\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(y_label)\n",
    "    plt.legend(loc='upper left')\n",
    "    \n",
    "    if save == True:\n",
    "        plt.savefig(str(path)+'\\\\' + str(save_as) + '.png')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To increase the amount of training data, build an image generator using data augmentation\n",
    "aug_gen = ImageDataGenerator(rotation_range=30, width_shift_range=0.1,\n",
    "                            height_shift_range=0.1, shear_range=0.2,\n",
    "                            zoom_range=0.2, horizontal_flip=True, fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize global training variables\n",
    "\n",
    "EPOCHS = 30\n",
    "#Learning rate\n",
    "#LR = 1e-3\n",
    "#Batch Size\n",
    "BS = 15\n",
    "path_to_img = 'C:\\\\Users\\\\jltsa\\\\Desktop\\\\Project_2\\\\images'\n",
    "path_to_models = 'models\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\jltsa\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "sgd1 = SGD(lr=0.01, momentum=0.5, nesterov=True)\n",
    "sgd2 = SGD(lr=0.01, momentum=0.8, nesterov=True)\n",
    "sgd3 = SGD(lr=0.001, momentum=0.5, nesterov=True)\n",
    "sgd4 = SGD(lr=0.001, momentum=0.8, nesterov=True)\n",
    "sgd5 = SGD(lr=0.0001, momentum=0.5, nesterov=True)\n",
    "sgd6 = SGD(lr=0.0001, momentum=0.8, nesterov=True)\n",
    "\n",
    "opts = [sgd1, sgd2, sgd3, sgd4, sgd5, sgd6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = []\n",
    "val_loss = []\n",
    "acc = []\n",
    "val_acc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(im_path, model_path, optimizer, weights):\n",
    "\n",
    "    data, labels = pre_process(227, 227, im_path)\n",
    "    #Split data into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.25, random_state=42)\n",
    "    #One hot encoding\n",
    "    y_train = to_categorical(y_train)\n",
    "    y_test = to_categorical(y_test)\n",
    "    \n",
    "\n",
    "    #create a checkpoint to store the best weights of the model\n",
    "    #to use these weights later, initialize the same model architecture that the weights were trained from\n",
    "    #then call model.load_weights('best_weights_alex.h5')\n",
    "    #can make predictions model.predict_classes(data)\n",
    "    model_path = model_path+weights+\".h5\"\n",
    "    checkpoint = ModelCheckpoint(model_path, monitor='val_loss', save_best_only=True)\n",
    "    #add early stopping if \n",
    "    earlystop = EarlyStopping(monitor='val_loss', patience=4)\n",
    "        \n",
    "    callbacks_list=[checkpoint, earlystop]\n",
    "        \n",
    "    #Initialize model\n",
    "    model = AlexNet.build(width=227, height=227, depth=3, classes=3)\n",
    "    #opt_alex = Adam(lr=LR, decay=LR / EPOCHS)\n",
    "\n",
    "    #if model has 2 classes use loss='binary_crossentropy'\n",
    "    #AlexNet performs better with 'sgd' optimizer\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    #fit model using a generator to increase variation and amount of data\n",
    "    model.fit_generator(aug_gen.flow(X_train, y_train, batch_size=BS),\n",
    "                    validation_data=(X_test, y_test), steps_per_epoch=len(X_train // BS),\n",
    "                    epochs=EPOCHS, callbacks=callbacks_list, verbose=1)\n",
    "    #save history for loss and val scores\n",
    "    loss.append(model.history.history['loss'])\n",
    "    val_loss.append(model.history.history['val_loss'])\n",
    "    acc.append(model.history.history['acc'])\n",
    "    val_acc.append(model.history.history['val_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\jltsa\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\Users\\jltsa\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/30\n",
      "543/543 [==============================] - 94s 173ms/step - loss: 1.0065 - acc: 0.4595 - val_loss: 0.9848 - val_acc: 0.4807\n",
      "Epoch 2/30\n",
      "543/543 [==============================] - 86s 159ms/step - loss: 0.9723 - acc: 0.5150 - val_loss: 0.9378 - val_acc: 0.5414\n",
      "Epoch 3/30\n",
      "543/543 [==============================] - 87s 160ms/step - loss: 0.9468 - acc: 0.5353 - val_loss: 0.9333 - val_acc: 0.5635\n",
      "Epoch 4/30\n",
      "543/543 [==============================] - 87s 160ms/step - loss: 0.9358 - acc: 0.5393 - val_loss: 0.9363 - val_acc: 0.5414\n",
      "Epoch 5/30\n",
      "543/543 [==============================] - 87s 159ms/step - loss: 0.9004 - acc: 0.5580 - val_loss: 0.9114 - val_acc: 0.5525\n",
      "Epoch 6/30\n",
      "543/543 [==============================] - 87s 160ms/step - loss: 0.8644 - acc: 0.5788 - val_loss: 0.8729 - val_acc: 0.6077\n",
      "Epoch 7/30\n",
      "543/543 [==============================] - 87s 160ms/step - loss: 0.8200 - acc: 0.6037 - val_loss: 0.9299 - val_acc: 0.5691\n",
      "Epoch 8/30\n",
      "543/543 [==============================] - 86s 159ms/step - loss: 0.7882 - acc: 0.6279 - val_loss: 0.8879 - val_acc: 0.6133\n",
      "Epoch 9/30\n",
      "543/543 [==============================] - 85s 157ms/step - loss: 0.7364 - acc: 0.6657 - val_loss: 0.9765 - val_acc: 0.6740\n"
     ]
    }
   ],
   "source": [
    "train_model(path_to_img, path_to_models, sgd2, \"SGD2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "i =[]\n",
    "i.append(val_acc[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.5138121709968504,\n",
       "  0.508287306977899,\n",
       "  0.5580110661891284,\n",
       "  0.6022099635219047,\n",
       "  0.5745856557761767,\n",
       "  0.6298342737374385,\n",
       "  0.5801105148555166,\n",
       "  0.6132596866201959,\n",
       "  0.46961327613387976]]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('metrics\\\\'+'val_acc.pickle', 'rb') as file:\n",
    "    e = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.5138121709968504,\n",
       "  0.508287306977899,\n",
       "  0.5580110661891284,\n",
       "  0.6022099635219047,\n",
       "  0.5745856557761767,\n",
       "  0.6298342737374385,\n",
       "  0.5801105148555166,\n",
       "  0.6132596866201959,\n",
       "  0.46961327613387976]]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "e.append(val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.5138121709968504,\n",
       "  0.508287306977899,\n",
       "  0.5580110661891284,\n",
       "  0.6022099635219047,\n",
       "  0.5745856557761767,\n",
       "  0.6298342737374385,\n",
       "  0.5801105148555166,\n",
       "  0.6132596866201959,\n",
       "  0.46961327613387976],\n",
       " [[0.48066299182275385,\n",
       "   0.5414364812123841,\n",
       "   0.563535927738274,\n",
       "   0.5414364812123841,\n",
       "   0.5524862067804811,\n",
       "   0.6077348222719372,\n",
       "   0.5690607864883065,\n",
       "   0.6132596866201959,\n",
       "   0.6740331682711016]]]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('metrics\\\\'+'val_acc.pickle', 'wb') as file:\n",
    "            pickle.dump(e, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.5138121709968504,\n",
       "  0.508287306977899,\n",
       "  0.5580110661891284,\n",
       "  0.6022099635219047,\n",
       "  0.5745856557761767,\n",
       "  0.6298342737374385,\n",
       "  0.5801105148555166,\n",
       "  0.6132596866201959,\n",
       "  0.46961327613387976],\n",
       " [[0.48066299182275385,\n",
       "   0.5414364812123841,\n",
       "   0.563535927738274,\n",
       "   0.5414364812123841,\n",
       "   0.5524862067804811,\n",
       "   0.6077348222719372,\n",
       "   0.5690607864883065,\n",
       "   0.6132596866201959,\n",
       "   0.6740331682711016]]]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('metrics\\\\'+'val_acc.pickle', 'rb') as file:\n",
    "    e = pickle.load(file)\n",
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
